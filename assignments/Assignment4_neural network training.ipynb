{"cells":[{"cell_type":"markdown","source":["# Assigment 4\n","\n","\n","> Group:12 YanpingTang, Guangyu Ma, Weiyou Wang\n","\n"],"metadata":{"id":"2KKe10sDlBRc"},"id":"2KKe10sDlBRc"},{"cell_type":"code","execution_count":3,"id":"5f0fea34","metadata":{"id":"5f0fea34","executionInfo":{"status":"ok","timestamp":1740740783913,"user_tz":-60,"elapsed":5348,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch"]},{"cell_type":"markdown","id":"56578f6f","metadata":{"id":"56578f6f"},"source":["# Task 1\n","\n","**For Colab users:** the following command downloads the first CSV file."]},{"cell_type":"code","source":["!wget --no-check-certificate https://www.cse.chalmers.se/~richajo/dit866/assignments/a4/data/a4_synthetic.csv"],"metadata":{"id":"0bOXEcNYiW-A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740740784804,"user_tz":-60,"elapsed":894,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}},"outputId":"e2bb5c74-0d96-4ffb-aa8e-c2fb7c5038a5"},"id":"0bOXEcNYiW-A","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-02-28 11:06:22--  https://www.cse.chalmers.se/~richajo/dit866/assignments/a4/data/a4_synthetic.csv\n","Resolving www.cse.chalmers.se (www.cse.chalmers.se)... 129.16.222.93\n","Connecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.222.93|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5874 (5.7K) [text/csv]\n","Saving to: ‘a4_synthetic.csv’\n","\n","a4_synthetic.csv    100%[===================>]   5.74K  --.-KB/s    in 0s      \n","\n","2025-02-28 11:06:23 (1.61 GB/s) - ‘a4_synthetic.csv’ saved [5874/5874]\n","\n"]}]},{"cell_type":"markdown","id":"1761f4d2","metadata":{"id":"1761f4d2"},"source":["Loading the synthetic dataset."]},{"cell_type":"code","execution_count":9,"id":"aacba01e","metadata":{"id":"aacba01e","executionInfo":{"status":"ok","timestamp":1740740808431,"user_tz":-60,"elapsed":24,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}}},"outputs":[],"source":["# You may need to edit the path, depending on where you put the files.\n","data = pd.read_csv('a4_synthetic.csv')\n","\n","X = data.drop(columns='y').to_numpy()\n","Y = data.y.to_numpy()"]},{"cell_type":"markdown","id":"b166d6ba","metadata":{"id":"b166d6ba"},"source":["Training a linear regression model for this synthetic dataset."]},{"cell_type":"code","execution_count":10,"id":"111ff34b","metadata":{"id":"111ff34b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740740810799,"user_tz":-60,"elapsed":361,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}},"outputId":"cd48f2bd-625d-4fac-b5f4-a65b9a4d2a99"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: MSE = 0.7999662599533621\n","Epoch 2: MSE = 0.017392394375863204\n","Epoch 3: MSE = 0.009377417723923998\n","Epoch 4: MSE = 0.009355327363746255\n","Epoch 5: MSE = 0.00936544022662929\n","Epoch 6: MSE = 0.009366988615365699\n","Epoch 7: MSE = 0.009367207542284177\n","Epoch 8: MSE = 0.009367238225554502\n","Epoch 9: MSE = 0.00936724472411015\n","Epoch 10: MSE = 0.009367244635918724\n"]}],"source":["np.random.seed(1)\n","\n","w_init = np.random.normal(size=(2, 1))\n","b_init = np.random.normal(size=(1, 1))\n","\n","# We just declare the parameter tensors. Do not use nn.Linear.\n","w = torch.tensor(w_init, dtype=torch.float32, requires_grad=True)# TODO: a tensor initialized as w_init\n","b = torch.tensor(b_init, dtype=torch.float32, requires_grad=True)# TODO: a tensor initialized as b_init\n","\n","eta = 1e-2\n","opt = opt = torch.optim.SGD([w, b], lr=eta)# TODO: a SGD optimizer with a learning rate of eta\n","\n","for i in range(10):\n","\n","    sum_err = 0\n","\n","    for row in range(X.shape[0]):\n","        x = torch.tensor(X[[row], :]).float()\n","        y = torch.tensor(Y[[row]]).float()\n","\n","        # Forward pass.\n","        y_pred = x @ w + b# TODO: compute predicted value for x\n","        err = (y_pred - y) ** 2   # TODO: compute squared error loss\n","\n","        # Backward and update.\n","        # TODO: compute gradients and then update the model.\n","        opt.zero_grad()\n","        err.backward()\n","        opt.step()\n","        # For statistics.\n","        sum_err += err.item()\n","\n","    mse = sum_err / X.shape[0]\n","    print(f'Epoch {i+1}: MSE =', mse)"]},{"cell_type":"markdown","id":"ee3f221d","metadata":{"id":"ee3f221d"},"source":["# Task 2"]},{"cell_type":"code","execution_count":11,"id":"56be71d4","metadata":{"id":"56be71d4","executionInfo":{"status":"ok","timestamp":1740740818138,"user_tz":-60,"elapsed":43,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}}},"outputs":[],"source":["class Tensor:\n","\n","    # Constructor. Just store the input values.\n","    def __init__(self, data, requires_grad=False, grad_fn=None):\n","        self.data = data\n","        self.shape = data.shape\n","        self.grad_fn = grad_fn\n","        self.requires_grad = requires_grad\n","        self.grad = None\n","\n","    # So that we can print the object or show it in a notebook cell.\n","    def __repr__(self):\n","        dstr = repr(self.data)\n","        if self.requires_grad:\n","            gstr = ', requires_grad=True'\n","        elif self.grad_fn is not None:\n","            gstr = f', grad_fn={self.grad_fn}'\n","        else:\n","            gstr = ''\n","        return f'Tensor({dstr}{gstr})'\n","\n","    # Extract one numerical value from this tensor.\n","    def item(self):\n","        return self.data.item()\n","\n","    # YOUR WORK WILL BE DONE BELOW\n","\n","    # For Task 2:\n","\n","    # Operator +\n","    def __add__(self, right):\n","        # Call the helper function defined below.\n","        return addition(self, right)\n","\n","    # Operator -\n","    def __sub__(self, right):\n","        newData = self.data-right.data\n","        grad_fn = SubNode(self, right)\n","        return Tensor(newData,grad_fn=grad_fn,requires_grad=True)\n","\n","    # Operator @\n","    def __matmul__(self, right):\n","        newData = self.data@right.data\n","        grad_fn = MatMulNode(self, right)\n","        return Tensor(newData,grad_fn=grad_fn,requires_grad=True)\n","\n","    # Operator **\n","    def __pow__(self, right):\n","        # NOTE! We are assuming that right is an integer here, not a Tensor!\n","        if not isinstance(right, int):\n","            raise Exception('only integers allowed')\n","        if right < 2:\n","            raise Exception('power must be >= 2')\n","        grad_fn = PowNode(self, right)\n","        newData = self.data**right\n","        return Tensor(newData,grad_fn=grad_fn,requires_grad=True)\n","\n","\n","    # Backward computations. Will be implemented in Task 4.\n","    def backward(self, grad_output=None):\n","        # We first check if this tensor has a grad_fn: that is, one of the\n","        # nodes that you defined in Task 3.\n","        if self.grad_fn is not None:\n","            # If grad_fn is defined, we have computed this tensor using some operation.\n","            if grad_output is None:\n","                # This is the starting point of the backward computation.\n","                # This will typically be the tensor storing the output of\n","                # the loss function, on which we have called .backward()\n","                # in the training loop.\n","                grad_output = 1.0\n","                # This is an intermediate node in the computational graph.\n","                # This corresponds to any intermediate computation, such as\n","                # a hidden layer.\n","            self.grad_fn.backward(grad_output)\n","        else:\n","            # If grad_fn is not defined, this is an endpoint in the computational\n","            # graph: learnable model parameters or input data.\n","            if self.requires_grad:\n","                # This tensor *requires* a gradient to be computed. This will\n","                # typically be a tensor that holds learnable parameters.\n","                if self.grad is None:\n","                    self.grad = grad_output\n","                else:\n","                    self.grad += grad_output\n","            else:\n","                # This tensor *does not require* a gradient to be computed. This\n","                # will typically be a tensor holding input data.\n","                pass\n","\n","\n","# A small utility where we simply create a Tensor object. We use this to\n","# mimic torch.tensor.\n","def tensor(data, requires_grad=False):\n","    return Tensor(data, requires_grad)\n","\n","# We define helper functions to implement the various arithmetic operations.\n","\n","def tanh(t):\n","\n","    out_data = np.tanh(t.data)\n","    requires_grad = t.requires_grad\n","    grad_fn = TanhNode(t)\n","    return Tensor(out_data, requires_grad=requires_grad, grad_fn=grad_fn)\n","\n","def sigmoid(t):\n","    out_data = 1 / (1 + np.exp(-t.data))\n","    requires_grad = t.requires_grad\n","    grad_fn = SigmoidNode(t)\n","    return Tensor(out_data, requires_grad=requires_grad, grad_fn=grad_fn)\n","\n","def bce_loss(pred, target):\n","\n","    p = pred.data\n","    t = target.data\n","    loss_val = - (t * np.log(p + 1e-8) + (1 - t) * np.log(1 - p + 1e-8))\n","    loss_tensor = Tensor(np.array(loss_val), requires_grad=pred.requires_grad, grad_fn=BCE_LossNode(pred, target))\n","    return loss_tensor\n","# This function takes two tensors as input, and returns a new tensor holding\n","# the result of an element-wise addition on the two input tensors.\n","def addition(left, right):\n","    new_data = left.data + right.data\n","    grad_fn = AdditionNode(left, right) # TODO build a node in the computational graph (Task 3)\n","    return Tensor(new_data,grad_fn=grad_fn,requires_grad=True)"]},{"cell_type":"markdown","id":"36d0f04c","metadata":{"id":"36d0f04c"},"source":["Some sanity checks."]},{"cell_type":"code","execution_count":14,"id":"f2014827","metadata":{"id":"f2014827","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740740849150,"user_tz":-60,"elapsed":20,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}},"outputId":"29b7b498-fb92-4462-ac4c-00b89ff7b076"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test of addition: [[2. 3.]] + [[1. 4.]] = [[3. 7.]]\n","Test of subtraction: [[2. 3.]] - [[1. 4.]] = [[ 1. -1.]]\n","Test of power: [[1. 4.]] ** 2 = [[ 1. 16.]]\n","Test of matrix multiplication: [[2. 3.]] @ [[-1. ]\n"," [ 1.2]] = [[1.6]]\n"]}],"source":["# Two tensors holding row vectors.\n","x1 = tensor(np.array([[2.0, 3.0]]))\n","x2 = tensor(np.array([[1.0, 4.0]]))\n","# A tensors holding a column vector.\n","w = tensor(np.array([[-1.0], [1.2]]))\n","\n","# Test the arithmetic operations.\n","test_plus = x1 + x2\n","test_minus = x1 - x2\n","test_power = x2 ** 2\n","test_matmul = x1 @ w\n","\n","print(f'Test of addition: {x1.data} + {x2.data} = {test_plus.data}')\n","print(f'Test of subtraction: {x1.data} - {x2.data} = {test_minus.data}')\n","print(f'Test of power: {x2.data} ** 2 = {test_power.data}')\n","print(f'Test of matrix multiplication: {x1.data} @ {w.data} = {test_matmul.data}')\n","\n","# Check that the results are as expected. Will crash if there is a miscalculation.\n","assert(np.allclose(test_plus.data, np.array([[3.0, 7.0]])))\n","assert(np.allclose(test_minus.data, np.array([[1.0, -1.0]])))\n","assert(np.allclose(test_power.data, np.array([[1.0, 16.0]])))\n","assert(np.allclose(test_matmul.data, np.array([[1.6]])))"]},{"cell_type":"markdown","id":"7c645c32","metadata":{"id":"7c645c32"},"source":["# Tasks 3 and 4"]},{"cell_type":"code","execution_count":13,"id":"9133db2f","metadata":{"id":"9133db2f","executionInfo":{"status":"ok","timestamp":1740740845490,"user_tz":-60,"elapsed":48,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}}},"outputs":[],"source":["class Node:\n","    def __init__(self):\n","        pass\n","\n","    def backward(self, grad_output):\n","        raise NotImplementedError('Unimplemented')\n","\n","    def __repr__(self):\n","        return str(type(self))\n","\n","\n","class AdditionNode(Node):\n","    def __init__(self, left, right):\n","        self.left = left\n","        self.right = right\n","\n","    def backward(self, grad_output):\n","        # TODO: in Task 4, implement backward step for the addition operation.\n","        if self.left.requires_grad:\n","            self.left.backward(grad_output)\n","        if self.right.requires_grad:\n","            self.right.backward(grad_output)\n","\n","class SubNode(Node):\n","    def __init__(self, left, right):\n","        self.left = left\n","        self.right = right\n","\n","    def backward(self, grad_output):\n","        # TODO: in Task 4, implement backward step for the addition operation.\n","        if self.left.requires_grad:\n","            self.left.backward(grad_output)\n","        if self.right.requires_grad:\n","            self.right.backward(-grad_output)\n","\n","class MatMulNode(Node):\n","    def __init__(self, left, right):\n","        self.left = left\n","        self.right = right\n","\n","    def backward(self, grad_output):\n","        # TODO: in Task 4, implement backward step for the addition operation.\n","        if self.left.requires_grad:\n","            self.left.backward(grad_output @ self.right.data.T)  # dL/dA = dL/dC @ B.T\n","        if self.right.requires_grad:\n","            self.right.backward(self.left.data.T @ grad_output)  # dL/dB = A.T @ dL/dC\n","\n","class PowNode(Node):\n","    def __init__(self, left, right):\n","        self.left = left\n","        self.right = right\n","\n","    def backward(self, grad_output):\n","        if self.left.requires_grad:\n","            self.left.backward(self.right * (self.left.data ** (self.right - 1)) * grad_output)\n","\n","class TanhNode(Node):\n","    def __init__(self, input_tensor):\n","        self.input_tensor = input_tensor\n","        self.out = np.tanh(input_tensor.data)\n","\n","    def backward(self, grad_output):\n","\n","        grad_input = grad_output * (1 - self.out**2)\n","        if self.input_tensor.requires_grad:\n","            self.input_tensor.backward(grad_input)\n","\n","\n","\n","class SigmoidNode(Node):\n","    def __init__(self, input_tensor):\n","        self.input_tensor = input_tensor\n","        self.out = 1 / (1 + np.exp(-input_tensor.data))\n","\n","    def backward(self, grad_output):\n","\n","        grad_input = grad_output * self.out * (1 - self.out)\n","        if self.input_tensor.requires_grad:\n","            self.input_tensor.backward(grad_input)\n","\n","\n","class BCE_LossNode(Node):\n","    def __init__(self, pred, target):\n","        self.pred = pred\n","        self.target = target\n","\n","    def backward(self, grad_output):\n","        p = self.pred.data\n","        t = self.target.data\n","        grad_pred = (-t / (p + 1e-8) + (1 - t) / (1 - p + 1e-8)) * grad_output\n","        if self.pred.requires_grad:\n","            self.pred.backward(grad_pred)"]},{"cell_type":"markdown","id":"9cc1bb77-e869-4e08-8996-3674eed101e6","metadata":{"id":"9cc1bb77-e869-4e08-8996-3674eed101e6"},"source":["Sanity check for Task 3."]},{"cell_type":"code","execution_count":15,"id":"f3276aba-4def-421b-b12e-bf0d7120f19e","metadata":{"id":"f3276aba-4def-421b-b12e-bf0d7120f19e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740740856225,"user_tz":-60,"elapsed":51,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}},"outputId":"223188cb-087d-4fb3-c7fe-38f5edafd3f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Computational graph top node after x + w1 + w2: <class '__main__.AdditionNode'>\n"]}],"source":["x = tensor(np.array([[2.0, 3.0]]))\n","w1 = tensor(np.array([[1.0, 4.0]]), requires_grad=True)\n","w2 = tensor(np.array([[3.0, -1.0]]), requires_grad=True)\n","\n","test_graph = x + w1 + w2\n","\n","print('Computational graph top node after x + w1 + w2:', test_graph.grad_fn)\n","\n","assert(isinstance(test_graph.grad_fn, AdditionNode))\n","assert(test_graph.grad_fn.right is w2)\n","assert(test_graph.grad_fn.left.grad_fn.left is x)\n","assert(test_graph.grad_fn.left.grad_fn.right is w1)"]},{"cell_type":"markdown","id":"529a9bfb-ea55-4bce-9356-4956316e1904","metadata":{"id":"529a9bfb-ea55-4bce-9356-4956316e1904"},"source":["Sanity check for Task 4."]},{"cell_type":"code","execution_count":16,"id":"32687661-a67d-4bef-9a90-7dabb93380a2","metadata":{"id":"32687661-a67d-4bef-9a90-7dabb93380a2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740740862803,"user_tz":-60,"elapsed":13,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}},"outputId":"1781a7b4-9886-4083-cc46-81080dd3bb6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient of loss w.r.t. w =\n"," [[5.6]\n"," [8.4]]\n"]}],"source":["x = tensor(np.array([[2.0, 3.0]]))\n","w = tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n","y = tensor(np.array([[0.2]]))\n","\n","# We could as well write simply loss = (x @ w - y)**2\n","# We break it down into steps here if you need to debug.\n","\n","model_out = x @ w\n","diff = model_out - y\n","loss = diff ** 2\n","\n","loss.backward()\n","\n","print('Gradient of loss w.r.t. w =\\n', w.grad)\n","\n","assert(np.allclose(w.grad, np.array([[5.6], [8.4]])))\n","assert(x.grad is None)\n","assert(y.grad is None)"]},{"cell_type":"markdown","id":"105ed8e2","metadata":{"id":"105ed8e2"},"source":["An equivalent cell using PyTorch code. Your implementation should give the same result for `w.grad`."]},{"cell_type":"code","execution_count":17,"id":"e72a5687","metadata":{"id":"e72a5687","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740740867408,"user_tz":-60,"elapsed":61,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}},"outputId":"4ca5146d-3fa4-468c-becf-bc63debdf5a7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[5.6000],\n","        [8.4000]], dtype=torch.float64)"]},"metadata":{},"execution_count":17}],"source":["pt_x = torch.tensor(np.array([[2.0, 3.0]]))\n","pt_w = torch.tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n","pt_y = torch.tensor(np.array([[0.2]]))\n","\n","pt_model_out = pt_x @ pt_w\n","pt_model_out.retain_grad() # Keep the gradient of intermediate nodes for debugging.\n","\n","pt_diff = pt_model_out - pt_y\n","pt_diff.retain_grad()\n","\n","pt_loss = pt_diff ** 2\n","pt_loss.retain_grad()\n","\n","pt_loss.backward()\n","pt_w.grad"]},{"cell_type":"markdown","id":"d0b5439b","metadata":{"id":"d0b5439b"},"source":["# Task 5"]},{"cell_type":"code","execution_count":18,"id":"0b03a8c5","metadata":{"id":"0b03a8c5","executionInfo":{"status":"ok","timestamp":1740740870539,"user_tz":-60,"elapsed":3,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}}},"outputs":[],"source":["class Optimizer:\n","    def __init__(self, params):\n","        self.params = params\n","\n","    def zero_grad(self):\n","        for p in self.params:\n","            p.grad = np.zeros_like(p.data)\n","\n","    def step(self):\n","        raise NotImplementedError('Unimplemented')\n","\n","\n","class SGD(Optimizer):\n","    def __init__(self, params, lr):\n","        super().__init__(params)\n","        self.lr = lr\n","\n","    def step(self):\n","        for p in self.params:\n","            if p.grad is not None:\n","                p.data -= self.lr * p.grad"]},{"cell_type":"code","source":["np.random.seed(1)\n","\n","w_init = np.random.normal(size=(2, 1))\n","b_init = np.random.normal(size=(1, 1))\n","\n","# We just declare the parameter tensors. Do not use nn.Linear.\n","w = tensor(w_init,requires_grad=True)# TODO: a tensor initialized as w_init\n","b = tensor(b_init,requires_grad=True)# TODO: a tensor initialized as b_init\n","\n","eta = 1e-2\n","opt = SGD([w, b], lr=eta)# TODO: a SGD optimizer with a learning rate of eta\n","\n","for i in range(10):\n","\n","    sum_err = 0\n","\n","    for row in range(X.shape[0]):\n","        x = tensor(X[[row], :])\n","        y = tensor(Y[[row]])\n","\n","        # Forward pass.\n","        y_pred = x @ w + b# TODO: compute predicted value for x\n","        err = (y_pred - y) ** 2   # TODO: compute squared error loss\n","\n","        # Backward and update.\n","        # TODO: compute gradients and then update the model.\n","        opt.zero_grad()\n","        err.backward()\n","        opt.step()\n","        # For statistics.\n","        sum_err += err.item()\n","\n","    mse = sum_err / X.shape[0]\n","    print(f'Epoch {i+1}: MSE =', mse)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UOxlRG9cADND","executionInfo":{"status":"ok","timestamp":1740740875457,"user_tz":-60,"elapsed":66,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}},"outputId":"5d9e1ccc-a24f-4122-ca6c-b61977ec84a1"},"id":"UOxlRG9cADND","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: MSE = 0.7999661130823178\n","Epoch 2: MSE = 0.017392390107906875\n","Epoch 3: MSE = 0.009377418010839892\n","Epoch 4: MSE = 0.009355326971438456\n","Epoch 5: MSE = 0.009365440968904256\n","Epoch 6: MSE = 0.009366989180952533\n","Epoch 7: MSE = 0.009367207398577986\n","Epoch 8: MSE = 0.009367238983974489\n","Epoch 9: MSE = 0.009367243704122532\n","Epoch 10: MSE = 0.009367244427185763\n"]}]},{"cell_type":"markdown","id":"28bef171","metadata":{"id":"28bef171"},"source":["# Task 6"]},{"cell_type":"markdown","source":["**For Colab users:** the following command downloads the second CSV file."],"metadata":{"id":"S8lzVCGmiJK-"},"id":"S8lzVCGmiJK-"},{"cell_type":"code","source":["!wget --no-check-certificate https://www.cse.chalmers.se/~richajo/dit866/assignments/a4/data/raisins.csv"],"metadata":{"id":"CUWHDZ5Ph5h5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740740881916,"user_tz":-60,"elapsed":1148,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}},"outputId":"015339e9-9d88-4e14-c638-c7805e249dac"},"id":"CUWHDZ5Ph5h5","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-02-28 11:07:59--  https://www.cse.chalmers.se/~richajo/dit866/assignments/a4/data/raisins.csv\n","Resolving www.cse.chalmers.se (www.cse.chalmers.se)... 129.16.222.93\n","Connecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.222.93|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 68230 (67K) [text/csv]\n","Saving to: ‘raisins.csv’\n","\n","raisins.csv         100%[===================>]  66.63K   256KB/s    in 0.3s    \n","\n","2025-02-28 11:08:00 (256 KB/s) - ‘raisins.csv’ saved [68230/68230]\n","\n"]}]},{"cell_type":"code","execution_count":21,"id":"da62980a","metadata":{"id":"da62980a","executionInfo":{"status":"ok","timestamp":1740740887400,"user_tz":-60,"elapsed":22,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}}},"outputs":[],"source":["from sklearn.preprocessing import scale\n","from sklearn.model_selection import train_test_split\n","\n","a4data = pd.read_csv('raisins.csv')\n","\n","X = scale(a4data.drop(columns='Class'))\n","Y = 1.0*(a4data.Class == 'Besni').to_numpy()\n","\n","np.random.seed(0)\n","shuffle = np.random.permutation(len(Y))\n","X = X[shuffle]\n","Y = Y[shuffle]\n","\n","Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, random_state=0, test_size=0.2)\n","Ytrain = Ytrain.reshape(-1, 1)\n","Ytest = Ytest.reshape(-1, 1)"]},{"cell_type":"code","execution_count":null,"id":"727929a2","metadata":{"id":"727929a2","executionInfo":{"status":"aborted","timestamp":1740740791923,"user_tz":-60,"elapsed":13438,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}}},"outputs":[],"source":["Xtrain.shape, Ytrain.shape"]},{"cell_type":"code","source":["d_in = Xtrain.shape[1]\n","d_hidden = 4\n","W1 = tensor(np.random.randn(d_in, d_hidden), requires_grad=True)\n","b1 = tensor(np.zeros((1, d_hidden)), requires_grad=True)\n","W2 = tensor(np.random.randn(d_hidden, 1), requires_grad=True)\n","b2 = tensor(np.zeros((1, 1)), requires_grad=True)\n","\n","def model(x):\n","    z1 = x @ W1 + b1\n","    a1 = tanh(z1)\n","\n","    z2 = a1 @ W2 + b2\n","    out = sigmoid(z2)\n","    return out\n","learning_rate = 0.01\n","epochs = 100\n","N = Xtrain.shape[0]\n","\n","for epoch in range(epochs):\n","    epoch_loss = 0.0\n","\n","    for i in range(N):\n","        x_i = tensor(Xtrain[i:i+1, :])\n","        y_i = tensor(Ytrain[i:i+1, :])\n","        pred = model(x_i)\n","        loss = bce_loss(pred, y_i)\n","        epoch_loss += loss.data.item()\n","        loss.backward()\n","\n","        for param in [W1, b1, W2, b2]:\n","            param.data = param.data - learning_rate * param.grad\n","            param.grad = None\n","    if (epoch + 1) % 10 == 0:\n","        print(f\"Epoch {epoch+1}, Loss: {epoch_loss / N:.4f}\")\n","\n","\n","correct = 0\n","N_test = Xtest.shape[0]\n","for i in range(N_test):\n","    x_i = tensor(Xtest[i:i+1, :])\n","    pred = model(x_i)\n","    pred_label = 1 if pred.data.item() > 0.5 else 0\n","    if pred_label == int(Ytest[i][0]):\n","        correct += 1\n","accuracy = correct / N_test\n","print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CK-oNCA9EYm3","executionInfo":{"status":"ok","timestamp":1740740899442,"user_tz":-60,"elapsed":8539,"user":{"displayName":"Forrest Ma","userId":"07198603373838697086"}},"outputId":"e2ccf6ae-3a4a-4dec-a3fe-c9ef0ce45322"},"id":"CK-oNCA9EYm3","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 10, Loss: 0.3429\n","Epoch 20, Loss: 0.3327\n","Epoch 30, Loss: 0.3254\n","Epoch 40, Loss: 0.3191\n","Epoch 50, Loss: 0.3139\n","Epoch 60, Loss: 0.3092\n","Epoch 70, Loss: 0.3055\n","Epoch 80, Loss: 0.3032\n","Epoch 90, Loss: 0.3017\n","Epoch 100, Loss: 0.3007\n","Test Accuracy: 85.56%\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[{"file_id":"1nmldhtLHTqNGuP3A5yfBV7st8k1AO5r4","timestamp":1740192769491}]}},"nbformat":4,"nbformat_minor":5}